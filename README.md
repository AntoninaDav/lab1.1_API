#Практическая работа 1. Сбор и анализ данных с использованием API

**Московский городской педагогический университет**

**Дисциплина:** Анализ больших данных и рыночных тенденций

**Направление:** Бизнес-информатика (магистратура)

**Вариант 5**

**ФИО студента:** Давидченко Антонина Сергеевна

**Преподаватель:** Босенко Тимур Муртазович

**Цель работы:**  освоение способов программного сбора и анализа данных из различных веб-источников с использованием API, а также применение полученных навыков для решения бизнес-задач, а также выполнение варианта 5, включающего анализ авторов, форков и географического распределения вакансий.

**ПО:** Python 3.x, Jupyter Notebook или любая IDE, Git.
**Библиотеки:** requests, pandas, matplotlib, seaborn, kaggle, PyGithub.

**Задание по вариантам**

1)Анализ авторов: найти топ-5 авторов (пользователей) по общему количеству голосов за их датасеты.

2)Анализ "форков": найти репозиторий tensorflow/tensorflow. Проанализировать последние 20 форков: кто и зачем их создает (анализ описаний).

3)География вакансий: сравнить количество вакансий по запросу "Big Data" в Москве и Санкт-Петербурге.


## Выводы по работе

В ходе выполнения данной практической работы были получены углубленные навыки программного сбора и анализа данных с использованием API трех различных платформ: Kaggle, GitHub и hh.ru. Мы научились проходить аутентификацию, отправлять параметризованные запросы, обрабатывать и структурировать полученные JSON-ответы, а также визуализировать результаты для их наглядной интерпретации.

**Основные полученные навыки:**
- Работа с `kaggle` API для анализа данных о соревнованиях.
- Использование `requests` для взаимодействия с REST API GitHub и hh.ru.
- Обработка и анализ данных с помощью библиотеки `pandas`.
- Визуализация результатов с помощью `matplotlib` и `seaborn`.

**Возникшие трудности и способы их решения:**
- **Ограничения API (Rate Limiting):** при частом обращении к API можно столкнуться с лимитами на количество запросов. Проблема решалась введением небольших пауз (`time.sleep()`) между запросами.
- **Неструктурированные данные:** данные, полученные через API, требовали предварительной очистки и преобразования (например, очистка призового фонда в Kaggle, приведение навыков к нижнему регистру в hh.ru), что подчеркивает важность этапа подготовки данных (Data Wrangling).

Данная работа наглядно демонстрирует, как с помощью API можно автоматизировать сбор актуальных данных и проводить на их основе прикладные бизнес-исследования, будь то анализ рыночных тенденций, оценка популярности технологий или мониторинг рынка труда.